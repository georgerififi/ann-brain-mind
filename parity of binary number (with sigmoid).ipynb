{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARALLEL DISTRIBUTED PROCESSING (Hinton) p.334\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(y):\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    # Make weight initiation repeatable, so that chance of initializing the weights \n",
    "    # closer to the optimum, doesn't affect the performance of the net.\n",
    "    # Feel free to comment out this line, and you will see in the plot, that the net \n",
    "    # will be converging in a  minimun prediction error, faster or slower, each time.\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        ######\n",
    "        # self ; the class object\n",
    "        # N ; len(data[0]) = 4 , where x the original data array mxN, NOT yet concatenated\n",
    "        ######\n",
    "        \n",
    "        self.activation = sigmoid\n",
    "        self.activation_derivative = sigmoid_derivative\n",
    "        self.arch = [N, N, 1]\n",
    "        self.layers = len(self.arch)\n",
    "        self.weights = []\n",
    "        for i in range(2):\n",
    "            w = 2*np.random.rand(self.arch[i]+1, self.arch[i+1])  -  1\n",
    "            self.weights.append(w) # [(N+1)xN, (N+1)x1]\n",
    "            \n",
    "    def _forward(self, x):\n",
    "        #####\n",
    "        # self ; class object\n",
    "        # x ; the concatenated data array mx(N+1), i.e. [[1 0 0 0 0]\n",
    "        #                                                [1 0 0 0 1]\n",
    "        #                                                [1 0 0 1 0]\n",
    "        #                                                [1 0 0 1 1]\n",
    "        #                                                ...\n",
    "        #                                                [1 1 1 1 1]]\n",
    "        ####\n",
    "        \n",
    "        # initialize list of arrays, each array mxN[i], the activations of the layer i. \n",
    "        a = []\n",
    "        \n",
    "        # 1st layer\n",
    "        a.append(x) # [array(m,(N+1))]\n",
    "        \n",
    "        # hidden layer\n",
    "        for i in range(len(self.weights)-1): # 0..(1-1) -> 0\n",
    "            z = np.dot(a[i], self.weights[i]) # m,(N+1) dot (N+1),N = m,N\n",
    "            a_temp = self.activation(z) # m,N\n",
    "            a_temp = np.concatenate((np.ones((1, a_temp.shape[0])).T, a_temp), axis=1) # m,1 concat m,N = m,(N+1)\n",
    "            a.append(a_temp) # [array(m,(N+1)), array(m,(N+1))]\n",
    "            \n",
    "        # last layer\n",
    "        z = np.dot(a[-1], self.weights[-1]) # m,N+1 dot N+1,1 = m,1\n",
    "        a_temp = self.activation(z) # m,1\n",
    "        a.append(a_temp)  \n",
    "        \n",
    "        return a # [array(m,N+1), array(m,N+1), array(m,1)]\n",
    "    \n",
    "    \n",
    "    def _backprop(self, a, labels, learning_rate=0.1):\n",
    "        #####\n",
    "        # self ; class object\n",
    "        # a ; the list of activation arrays - [array(m,(N+1)), array(m,(N+1)), array(m,1)] -.\n",
    "        # labels ; the (m,) array with the labels\n",
    "        # learning_rate ; 0.1\n",
    "        #####\n",
    "        \n",
    "        ## we use the chain rule ; delta_weight = dError/dz * dz/dweight\n",
    "        # more formally delta_weight[j] = a[j].T.dot(delta_vec[j]), where \n",
    "        # delta_vec[j] = dError/dz[j+1], and \n",
    "        # delta_vec = delta_vec.reverse()\n",
    "        \n",
    "        # initialize the delta_vec ; i.e. the derivatives [ dError/dz[2], dError/dz[1]] \n",
    "        delta_vec = []\n",
    "        \n",
    "        # last weight ; delta_vec[0] = dError/dz[2]\n",
    "        error = (labels - a[-1]) # m,1 - m,1 = m,1\n",
    "        error = error * self.activation_derivative(a[-1]) # m,1 * m,1 = m,1\n",
    "        delta_vec = [error] # [dError/dz[2]] of shape [(m,1)]\n",
    "        \n",
    "        # nextolast up to first weight array ; here there is only delta_vec[1] = dError/dz[1]\n",
    "        for i in range((self.layers-2), 0, -1): # (3-2)..1 -> 1..1 -> 1\n",
    "            error = delta_vec[-1].dot(self.weights[i][1:].T) # m,1 dot 1,N = m,N\n",
    "            error = error * self.activation_derivative(a[i][:,1:]) # m,N * m,N = m,N\n",
    "            delta_vec.append(error) # [dError/dz[2], dError/dz[1]] = [array(m,1), array(m,N)]\n",
    "            \n",
    "        # reverse [dError/dz[2], dError/dz[1]] -> [dError/dz[1], dError/dz[2]]\n",
    "        delta_vec.reverse() # [(m,N), (m,1)]\n",
    "        \n",
    "        # update rule\n",
    "        for i in range(len(self.weights)):\n",
    "            layer = a[i] # 1st layer ; m,N+1\n",
    "                         # 2nd layer ; m,N+1\n",
    "            delta = delta_vec[i] # 1st layer ; m,N \n",
    "                                 # 2nd layer ; m,1\n",
    "            self.weights[i] += learning_rate * layer.T.dot(delta) # 1st ; N+1,m dot m,N = N+1,N\n",
    "                                                                  # 2nd ; N+1,m dot m,1 = N+1,1\n",
    "    \n",
    "    def _fit(self, original_data, labels, learning_rate=0.1, epochs=3000):\n",
    "        #####\n",
    "        # self ; the class object \n",
    "        # original_data ; (m,N) array of all possible binary combinations (m = 2^N)\n",
    "        # labels ; (m,) array\n",
    "        # epochs ; i have already predefined the epochs to 3000 because that's the min  \n",
    "        #          number of epochs, the net needs to start to converge.\n",
    "        # learning_rate ; feel free to play with the learning rate\n",
    "        ######\n",
    "        \n",
    "        # add the bias unit to the input layer for all the examples ; (m,N) -> (m,N+1)\n",
    "        ones = np.ones((1, original_data.shape[0])) # 1,m\n",
    "        data = np.concatenate((ones.T, original_data), axis=1) # m,1 concat m,N -> m,N+1\n",
    "        \n",
    "        # keep track of prediction error in the plot prediction error, epochs) \n",
    "        er = []\n",
    "        \n",
    "        # reshape the labels (m,) -> m,1 \n",
    "        m = labels.shape[0]\n",
    "        labels = labels.reshape(m, 1)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # create a batch of 100 samples for the epoch. \n",
    "            sample = np.random.randint(data.shape[0], size=100) # (1,100) array with int's from 0..15\n",
    "            \n",
    "            # forward\n",
    "            input_ = data[sample]\n",
    "            a = self._forward(input_)\n",
    "            \n",
    "            # backprop\n",
    "            target = labels[sample]\n",
    "            self._backprop(a, target, learning_rate=0.2)\n",
    "            \n",
    "            # keep track of the error for the plot \n",
    "            err = 0\n",
    "            for i in range(original_data.shape[0]):\n",
    "                err += np.square(self.predict_single_data(original_data[i]) - labels[i])\n",
    "            er.append(err)\n",
    "            \n",
    "            if epoch % 1000 == 0:\n",
    "                print ('epoch is {}'.format(epoch))\n",
    "                print ('prediction error is :', err) \n",
    "        \n",
    "        # plot prediction error vs epoch\n",
    "        x_axis = [i for i in range(epochs)]\n",
    "        y_axis = [er[j] for j in range(epochs)]\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(x_axis, y_axis)\n",
    "        ax.set(xlabel='epoch', \n",
    "               ylabel='prediction error', \n",
    "               title='prediction error vs epoch')\n",
    "        plt.show()\n",
    "        \n",
    "        # Show the prediction results\n",
    "        print(\"Final predictions : is it even?\\n\")\n",
    "        for s in original_data:\n",
    "            prediction = nn.predict_single_data(s)\n",
    "            print(s, prediction, prediction<0.5) \n",
    "            \n",
    "        # print out the weights\n",
    "        print('weights are: \\n (1st line are the biases for the layer)\\n', self.weights)\n",
    "        \n",
    "    def predict_single_data(self, x):\n",
    "        ######\n",
    "        # x ; 1,N array input for example [1 0 1 1]\n",
    "        ######\n",
    "        \n",
    "        # add the bias unit\n",
    "        val = np.concatenate((np.ones(1).T, np.array(x))) # 1x4 -> 1x5\n",
    "        \n",
    "        for i in range(0, len(self.weights)):\n",
    "            val = self.activation(np.dot(val, self.weights[i])) # 1x5 dot 5x4 = 1x4, 1x5 dot 5x1=1x1\n",
    "            val = np.concatenate((np.ones(1).T, np.array(val))) # 1x5, 1x2\n",
    "        \n",
    "        return val[1] # val is concat'd  because of the last step, and we want the element that was already\n",
    "                      # there. concat adds the element in the 1st position, so we call the second position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the only parameter of the net. Try playing around with N, to see how, the length \n",
    "# of the binary input, affects the algorithm. Here i chose to train a net that predicts, wether\n",
    "# the 4bit entry has even (outputs 0) or odd (outputs 1), number of 1's.\n",
    "# for example [0 1 1 0] will output 0.\n",
    "N = 4\n",
    "# create the net\n",
    "m = np.power(2,N)\n",
    "nn = NeuralNetwork(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data : \n",
      " [[0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 1 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 1 0]\n",
      " [0 1 1 1]\n",
      " [1 0 0 0]\n",
      " [1 0 0 1]\n",
      " [1 0 1 0]\n",
      " [1 0 1 1]\n",
      " [1 1 0 0]\n",
      " [1 1 0 1]\n",
      " [1 1 1 0]\n",
      " [1 1 1 1]]\n",
      "<class 'numpy.ndarray'>\n",
      "labels : \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# create the dataset and the labels for N-length inputs\n",
    "data = list(map(list, product(range(0, 2), repeat=N)))\n",
    "labels = []\n",
    "for i in range (len(data)):\n",
    "    if data[i].count(1) % 2 == 0:\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "labels = labels.reshape(m, 1)\n",
    "\n",
    "print ('data : \\n', data)\n",
    "print (type(data))\n",
    "print ('labels : \\n',labels)\n",
    "print (type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is 0\n",
      "prediction error is : [ 6.25104355]\n",
      "epoch is 1000\n",
      "prediction error is : [ 4.07459572]\n",
      "epoch is 2000\n",
      "prediction error is : [ 3.87058581]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBUIJX0SkQPw/cHjpivSzYhIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x89012b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions : is it even?\n",
      "\n",
      "[0 0 0 0] 0.034000689397 True\n",
      "[0 0 0 1] 0.970422612396 False\n",
      "[0 0 1 0] 0.971036408151 False\n",
      "[0 0 1 1] 0.242494123022 True\n",
      "[0 1 0 0] 0.971076564941 False\n",
      "[0 1 0 1] 0.0277116722617 True\n",
      "[0 1 1 0] 0.0279195546925 True\n",
      "[0 1 1 1] 0.97433664121 False\n",
      "[1 0 0 0] 0.971382448002 False\n",
      "[1 0 0 1] 0.242507683477 True\n",
      "[1 0 1 0] 0.242610865262 True\n",
      "[1 0 1 1] 0.234888931809 True\n",
      "[1 1 0 0] 0.0277838889728 True\n",
      "[1 1 0 1] 0.974631430082 False\n",
      "[1 1 1 0] 0.975161999127 False\n",
      "[1 1 1 1] 0.243595696666 True\n",
      "weights are: \n",
      " (1st line are the biases for the layer)\n",
      " [array([[-3.3849764 , -2.15776822,  2.70453514,  6.46565081],\n",
      "       [-5.52860287, -1.4178881 , -6.63184703, -5.81075982],\n",
      "       [ 5.53489319, -0.8745304 ,  7.02224767,  5.96609816],\n",
      "       [-5.52253863, -1.03766761, -6.63871448, -5.81948515],\n",
      "       [-5.52483492, -1.19743626, -6.62327136, -5.83021933]]), array([[ -1.18125633],\n",
      "       [  8.70929221],\n",
      "       [  0.07476612],\n",
      "       [-10.56552939],\n",
      "       [  7.45592656]])]\n"
     ]
    }
   ],
   "source": [
    "nn._fit(data, labels, learning_rate=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is a step by step explanation of the `nn.fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the bias unit to the input layer for all the examples ; (m,N) -> (m,N+1)\n",
    "ones = np.ones((1, data.shape[0])) # 1,m\n",
    "data = np.concatenate((ones.T, data), axis=1) # m,1 concat m,N -> m,N+1\n",
    "        \n",
    "# keep track of prediction error in the plot prediction error, epochs) \n",
    "er = []\n",
    "# reshape the labels (m,) -> m,1 \n",
    "m = labels.shape[0]\n",
    "labels = labels.reshape(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    if epoch % 1000 == 0:\n",
    "        print ('epoch is {}'.format(epoch))\n",
    "            \n",
    "    # create 100 samples for the epoch \n",
    "    sample = np.random.randint(data.shape[0], size=100) # (1,100) array with int's from 0..15\n",
    "            \n",
    "    # forward\n",
    "    input_ = data[sample]\n",
    "    a = nn._forward(input_)\n",
    "            \n",
    "    # backprop\n",
    "    target = labels[sample]\n",
    "    nn._backprop(a, target, learning_rate=0.2)\n",
    "            \n",
    "    # keep track of the error for the plot \n",
    "    err = 0\n",
    "    for i in range(original_data.shape[0]):\n",
    "        err += np.square(nn.predict_single_data(original_data[i]) - labels[i])\n",
    "    er.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prediction error vs epoch\n",
    "x_axis = [i for i in range(epochs)]\n",
    "y_axis = [er[j] for j in range(epochs)]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis, y_axis)\n",
    "ax.set(xlabel='epoch', \n",
    "       ylabel='prediction error', \n",
    "       title='prediction error vs epoch')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
